# ğŸ§  GRPO Reasoning Model for GSM8K ğŸ“š

This project implements a Group Relative Preference Optimization (GRPO) approach to fine-tune the Qwen2.5-3B-Instruct language model for mathematical reasoning tasks using the GSM8K dataset.

## ğŸŒŸ Overview

The model is trained to solve grade school math problems by:
1. ğŸ” Generating step-by-step reasoning in a structured XML format
2. ğŸ¯ Providing a final numerical answer
3. âš™ï¸ Optimizing for both correctness and output format compliance using GRPO

Key features:
- ğŸš€ Uses Unsloth for efficient 4-bit quantization and LoRA adaptation
- ğŸ† Implements custom reward functions for answer correctness and XML formatting
- âš¡ Leverages vLLM for high-speed text generation
- ğŸ“Š Trained on the GSM8K dataset of 8.5K math word problems

## âš™ï¸ Installation

```bash
pip install -U unsloth transformers trl vllm
pip install --upgrade unsloth unsloth_zoo
```

##   Model Architecture

- **Base Model**: `Qwen/Qwen2.5-3B-Instruct` ğŸ¤–
- **LoRA Configuration**:
  - Rank (r): 16
  - Target Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj
- **Sequence Length**: 2048 tokens

## ğŸš‚ Training Configuration

```python
GRPOConfig(
    use_vllm = True,          # âš¡ Turbo-charged generation
    learning_rate = 5e-6,      # ğŸ“‰ Optimized learning rate
    per_device_train_batch_size = 1,
    num_generations = 3,       # ğŸ”„ Multiple generations per prompt
    max_prompt_length = 500,   # ğŸ“ Input constraints
    max_completion_length = 1500,
    max_steps = 150            # â±ï¸ Training duration
)
```

## ğŸ† Reward Functions

Two custom reward functions guide the training:

1. **âœ… Correctness Reward**:
   - Returns 1.0 if extracted answer matches gold answer
   - Returns 0.0 otherwise

2. **ğŸ“ Format Reward**:
   - Returns 0.5 if output strictly follows XML format
   - Returns 0.0 otherwise

```python
def extract_xml_answer(text: str) -> str:
    # âœ‚ï¸ Extracts content between <answer> tags

def correctness_reward_func(prompts, completions, answer, **kwargs):
    # ğŸ†š Compares model output with gold answers

def strict_format_reward_func(prompts, completions, **kwargs):
    # ğŸ” Validates XML structure using regex
```

## ğŸ“‚ Data Preparation

The GSM8K dataset is processed with:
- System prompt requiring XML-style output
- Question formatting as user input
- Answer extraction from original dataset

```python
SYSTEM_PROMPT = """
Respond in the format:
<reasoning>...</reasoning>
<answer>...</answer>
"""

def get_gsm8k_questions():
    # ğŸ“¥ Loads and preprocesses dataset
```

## ğŸš€ Usage

### Training
```python
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[correctness_reward_func, strict_format_reward_func],
    train_dataset=dataset
)
trainer.train()  # ğŸš‚ Start the training process!
```

### Inference
```python
# Generate response
text = tokenizer.apply_chat_template([...], tokenize=False)
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
output = model.fast_generate([text], sampling_params)[0].text

# Example output:
"""
<reasoning>
The name "Naveen" has 5 letters. 
Looking for 'n's: 
- First letter 'N' (capital) 
- Fifth letter 'n' (lowercase)
Total n's: 2
</reasoning>
<answer>
2
</answer>
"""
```

### ğŸ’¾ Saving/Loading LoRA Weights
```python
# Save trained adapter
model.save_lora("grpo_saved_lora")  # ğŸ’¾

# Load for inference
model.load_lora("grpo_saved_lora")  # ğŸ”„
```

## ğŸ“‚ File Structure
- `reasoning_model_grpo.py`: Main training/inference script ğŸ
- `outputs/`: Directory for training outputs ğŸ“
- `grpo_saved_lora/`: Saved LoRA adapter weights ğŸ’½

## ğŸ“¦ Dependencies
- Python 3.8+ ğŸ
- PyTorch ğŸ”¥
- transformers ğŸ¤—
- trl ğŸ§ª
- vllm âš¡
- unsloth ğŸ¦¥
- datasets ğŸ“Š

## ğŸ¯ Key Benefits
- âœ¨ 4-bit quantization reduces memory requirements
- ğŸ§  Structured reasoning improves answer accuracy
- âš¡ vLLM enables rapid generation during training
- ğŸ† Dual-reward system ensures both correctness and formatting

